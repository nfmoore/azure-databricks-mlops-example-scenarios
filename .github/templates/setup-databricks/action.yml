name: Setup Databricks

description: This workflow sets up a Databricks workspace.

inputs:
  bicep_output_variable_name:
    description: The name of the output variable from the Bicep deployment corresponding to the Databricks workspace.
    required: true

runs:
  using: "composite"

  steps:
    # Set Databricks host and token environment variables
    - name: Set Databricks environment variables
      uses: azure/cli@v2
      with:
        azcliversion: ${{ env.AZ_CLI_VERSION }}
        inlineScript: |
          echo "DATABRICKS_HOST=https://$(jq .properties.outputs.${{ inputs.bicep_output_variable_name }}.value \
            deployment-output.json -r)" >> $GITHUB_ENV

          echo "DATABRICKS_TOKEN=$(az account get-access-token \
            --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d | jq .accessToken -r)" >> $GITHUB_ENV

    # Install the Databricks CLI
    - name: Install Databricks CLI
      shell: bash
      run: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh

    # Configure the Databricks CLI
    - name: Databricks CLI config
      shell: bash
      run: |
        cat > ~/.databrickscfg << EOF 
        [DEFAULT] 
        host = $DATABRICKS_HOST
        token = $DATABRICKS_TOKEN
        EOF

    # Create Databricks cluster
    - name: Create Databricks cluster
      shell: bash
      run: |
        # Create artifacts directory
        mkdir artifacts

        # Create Databricks cluster
        curl -X POST "$DATABRICKS_HOST/api/2.0/clusters/create" \
          -H "Authorization: Bearer $DATABRICKS_TOKEN" \
          -d '{
              "num_workers": 0,
              "cluster_name": "default",
              "spark_version": "14.3.x-cpu-ml-scala2.12",
              "spark_conf": {
                  "spark.master": "local[*, 4]",
                  "spark.databricks.cluster.profile": "singleNode"
              },
              "azure_attributes": {
                  "first_on_demand": 1,
                  "availability": "ON_DEMAND_AZURE",
                  "spot_bid_max_price": -1
              },
              "node_type_id": "Standard_D4ads_v5",
              "driver_node_type_id": "Standard_D4ads_v5",
              "autotermination_minutes": 60,
              "enable_elastic_disk": true,
              "enable_local_disk_encryption": false,
              "runtime_engine": "STANDARD"
          }' > artifacts/cluster-output-${{ inputs.bicep_output_variable_name }}.json
          
          # Display cluster output
          cat artifacts/cluster-output-${{ inputs.bicep_output_variable_name }}.json

    # Set Databricks cluster id environment variable
    - name: Set Databricks clister id
      shell: bash
      run: |
        echo "DATABRICKS_CLUSTER_ID=$(jq .cluster_id artifacts/cluster-output-${{ inputs.bicep_output_variable_name }}.json -r)" >> $GITHUB_ENV

    # Upload files to DBFS
    - name: Upload data to dbfs
      shell: bash
      run: |
        databricks fs mkdir dbfs:/FileStore/data/credit-card-default-uci-curated
        databricks fs cp -r databricks/data/curated.csv dbfs:/FileStore/data/credit-card-default-uci-curated/01.csv
        databricks fs mkdir dbfs:/FileStore/data/credit-card-default-uci-inference
        databricks fs cp -r databricks/data/inference.csv dbfs:/FileStore/data/credit-card-default-uci-inference/01.csv

    # Trigger notebook to create external tables
    - name: Create external tables
      uses: databricks/run-notebook@v0
      with:
        databricks-host: ${{ env.DATABRICKS_HOST }}
        databricks-token: ${{ env.DATABRICKS_TOKEN }}
        existing-cluster-id: ${{ env.DATABRICKS_CLUSTER_ID }}
        local-notebook-path: databricks/src/00-create-external-table.ipynb
        notebook-params-json: >
          {
            "curated_path": "dbfs:/FileStore/data/credit-card-default-uci-curated",
            "inference_path": "dbfs:/FileStore/data/credit-card-default-uci-inference"
          }
